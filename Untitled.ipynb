{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1657a4f9-ee6e-4620-a8ea-c8167a702eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                               text\n",
      "0    fraud  hello, i m bank manager of SBI, ur debit card ...\n",
      "1    fraud  Todays Vodafone numbers ending with 4882 are s...\n",
      "2   normal               Please don't say like that. Hi hi hi\n",
      "3   normal                                         Thank you!\n",
      "4   normal  Oh that was a forwarded message. I thought you...\n",
      "5   normal  Got it. Seventeen pounds for seven hundred ml ...\n",
      "6   normal                             Me and him so funny...\n",
      "7   normal  Sweetheart, hope you are not having that kind ...\n",
      "8   normal  When you login date time... Dad fetching you h...\n",
      "9   normal               What will we do in the shower, baby?\n",
      "10   fraud      Your account is at risk! Share OTP to verify.\n",
      "11   fraud               Enter your PIN to claim your reward.\n",
      "12   fraud                              Give me your OTP now.\n",
      "13   fraud    We noticed unusual activity. Send your OTP now!\n",
      "14  normal  I had askd u a question some hours before. Its...\n",
      "label\n",
      "normal    5286\n",
      "fraud      642\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label                                               text\n",
      "0       1  hello, i m bank manager of SBI, ur debit card ...\n",
      "1       1  Todays Vodafone numbers ending with 4882 are s...\n",
      "2       0               Please don't say like that. Hi hi hi\n",
      "3       0                                         Thank you!\n",
      "4       0  Oh that was a forwarded message. I thought you...\n",
      "5       0  Got it. Seventeen pounds for seven hundred ml ...\n",
      "6       0                             Me and him so funny...\n",
      "7       0  Sweetheart, hope you are not having that kind ...\n",
      "8       0  When you login date time... Dad fetching you h...\n",
      "9       0               What will we do in the shower, baby?\n",
      "10      1      Your account is at risk! Share OTP to verify.\n",
      "11      1               Enter your PIN to claim your reward.\n",
      "12      1                              Give me your OTP now.\n",
      "13      1    We noticed unusual activity. Send your OTP now!\n",
      "14      0  I had askd u a question some hours before. Its...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gadek\\AppData\\Local\\Temp\\ipykernel_12844\\437046785.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['label'].replace([\"fraud\", \"normal\"], [1, 0], inplace=True)\n",
      "C:\\Users\\gadek\\AppData\\Local\\Temp\\ipykernel_12844\\437046785.py:27: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'].replace([\"fraud\", \"normal\"], [1, 0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9822934232715008\n",
      "Precision: 0.9831932773109243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1050\n",
      "           1       0.98      0.86      0.92       136\n",
      "\n",
      "    accuracy                           0.98      1186\n",
      "   macro avg       0.98      0.93      0.95      1186\n",
      "weighted avg       0.98      0.98      0.98      1186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "\n",
    "# Display first 5 rows\n",
    "print(df.head(15))\n",
    "\n",
    "# Count fraud vs normal calls\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Plot fraud vs normal calls\n",
    "plt.hist(df['label'], color='r')\n",
    "plt.title(\"Fraud Calls vs Normal Calls\")\n",
    "plt.xlabel(\"Fraud (1) vs Normal (0)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Convert 'label' to binary (1 = Fraud, 0 = Normal)\n",
    "df['label'].replace([\"fraud\", \"normal\"], [1, 0], inplace=True)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "# **âœ… Fix: Use TF-IDF Vectorizer for Text Processing**\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Bi-grams\n",
    "X = vectorizer.fit_transform(df['text'])  # Convert text into numerical features\n",
    "Y = df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "# Train Logistic Regression Model with class balancing\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print Accuracy Metrics\n",
    "print(\"Accuracy:\", metrics.accuracy_score(Y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(Y_test, y_pred, zero_division=1))\n",
    "print(metrics.classification_report(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7615db-859b-4150-b44d-720ace312db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“© Enter a message to test (or type 'exit' to quit):  tell your CVV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Prediction: Fraud\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“© Enter a message to test (or type 'exit' to quit):  CVV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Prediction: Normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“© Enter a message to test (or type 'exit' to quit):  Your\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Prediction: Fraud\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“© Enter a message to test (or type 'exit' to quit):  you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Prediction: Normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“© Enter a message to test (or type 'exit' to quit):  your\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Prediction: Fraud\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“© Enter a message to test (or type 'exit' to quit):  r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Prediction: Normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“© Enter a message to test (or type 'exit' to quit):  OTP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Prediction: Fraud\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"\\nðŸ“© Enter a message to test (or type 'exit' to quit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    # Ensure proper feature extraction\n",
    "    input_vectorized = vectorizer.transform([user_input.lower()])  # Lowercasing helps match training data\n",
    "    \n",
    "    prediction = model.predict(input_vectorized)[0]\n",
    "    print(\"ðŸ”¹ Prediction:\", \"Fraud\" if prediction == 1 else \"Normal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1488226f-3f16-4f5e-acb3-5be0a5ad4f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.48.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: torch in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.4.1+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\gadek\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d75e3-69fa-4049-bf12-741ead6b3f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a8ceaa-bdbd-4706-adcc-8107ec45ded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9418604651162791\n",
      "Precision: 0.9834710743801653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90        45\n",
      "           1       0.98      0.94      0.96       127\n",
      "\n",
      "    accuracy                           0.94       172\n",
      "   macro avg       0.91      0.95      0.93       172\n",
      "weighted avg       0.95      0.94      0.94       172\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gadek\\AppData\\Local\\Temp\\ipykernel_19884\\1965364631.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['label'].replace([\"fraud\", \"normal\"], [1, 0], inplace=True)\n",
      "C:\\Users\\gadek\\AppData\\Local\\Temp\\ipykernel_19884\\1965364631.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'].replace([\"fraud\", \"normal\"], [1, 0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"lowercase_dataset.csv\")\n",
    "\n",
    "# Convert 'label' to binary (1 = Fraud, 0 = Normal)\n",
    "df['label'].replace([\"fraud\", \"normal\"], [1, 0], inplace=True)\n",
    "\n",
    "# Separate fraud and normal data\n",
    "fraud_df = df[df['label'] == 1]\n",
    "normal_df = df[df['label'] == 0]\n",
    "\n",
    "# **âœ… Ensure 75% Fraud & 25% Normal**\n",
    "num_fraud = len(fraud_df)\n",
    "num_normal = int(num_fraud * (25 / 75))  # 25% of fraud count\n",
    "\n",
    "# Downsample normal data to match 75%-25% ratio\n",
    "normal_sampled = normal_df.sample(n=num_normal, random_state=42, replace=False)\n",
    "\n",
    "# Merge the datasets\n",
    "balanced_df = pd.concat([fraud_df, normal_sampled])\n",
    "\n",
    "# Shuffle dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# **âœ… TF-IDF Vectorization**\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Bi-grams\n",
    "X = vectorizer.fit_transform(balanced_df['text'])  # Convert text into numerical features\n",
    "Y = balanced_df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# **âœ… Train Logistic Regression Model with Class Balancing**\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print Accuracy Metrics\n",
    "print(\"Accuracy:\", metrics.accuracy_score(Y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(Y_test, y_pred, zero_division=1))\n",
    "print(metrics.classification_report(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4690b8-7931-41d8-8637-ef0df66d5ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and vectorizer saved successfully!\n",
      "Accuracy: 0.9302325581395349\n",
      "Precision: 0.9831932773109243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88        45\n",
      "           1       0.98      0.92      0.95       127\n",
      "\n",
      "    accuracy                           0.93       172\n",
      "   macro avg       0.90      0.94      0.91       172\n",
      "weighted avg       0.94      0.93      0.93       172\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gadek\\AppData\\Local\\Temp\\ipykernel_26812\\2442619899.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['label'].replace([\"fraud\", \"normal\"], [1, 0], inplace=True)\n",
      "C:\\Users\\gadek\\AppData\\Local\\Temp\\ipykernel_26812\\2442619899.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'].replace([\"fraud\", \"normal\"], [1, 0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"S:\\ScamShield\\ScamScamShieldAI\\preprocessed_dataset.csv\")\n",
    "\n",
    "# Convert 'label' to binary (1 = Fraud, 0 = Normal)\n",
    "df['label'].replace([\"fraud\", \"normal\"], [1, 0], inplace=True)\n",
    "\n",
    "# Separate fraud and normal data\n",
    "fraud_df = df[df['label'] == 1]\n",
    "normal_df = df[df['label'] == 0]\n",
    "\n",
    "# **âœ… Ensure 75% Fraud & 25% Normal**\n",
    "num_fraud = len(fraud_df)\n",
    "num_normal = int(num_fraud * (25 / 75))  # 25% of fraud count\n",
    "\n",
    "# Downsample normal data to match 75%-25% ratio\n",
    "normal_sampled = normal_df.sample(n=num_normal, random_state=42, replace=False)\n",
    "\n",
    "# Merge the datasets\n",
    "balanced_df = pd.concat([fraud_df, normal_sampled])\n",
    "\n",
    "# Shuffle dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# **âœ… TF-IDF Vectorization**\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Bi-grams\n",
    "X = vectorizer.fit_transform(balanced_df['text'])  # Convert text into numerical features\n",
    "Y = balanced_df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# **âœ… Train Logistic Regression Model with Class Balancing**\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# **âœ… Save the Model**\n",
    "joblib.dump(model, \"finaltrainmodel.pkl\")  # Save trained model\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")  # Save TF-IDF vectorizer\n",
    "\n",
    "print(\"âœ… Model and vectorizer saved successfully!\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print Accuracy Metrics\n",
    "print(\"Accuracy:\", metrics.accuracy_score(Y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(Y_test, y_pred, zero_division=1))\n",
    "print(metrics.classification_report(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00e19526-bf0f-45c3-a220-c31b7c6e4451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       hello, i m bank manager of SBI, ur debit card ...\n",
      "1       Todays Vodafone numbers ending with 4882 are s...\n",
      "2                    Please don't say like that. Hi hi hi\n",
      "3                                              Thank you!\n",
      "4       Oh that was a forwarded message. I thought you...\n",
      "                              ...                        \n",
      "5923    to get 1000 INR voucher please call on 8898655...\n",
      "5924    to get free access of google cloud account hit...\n",
      "5925    to get free AWS cloud account hit on given mes...\n",
      "5926    to get free access of Microsoft Azure hit on g...\n",
      "5927    hello sir, we are from your bank have you fill...\n",
      "Name: text, Length: 5928, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gadek/nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 32\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Convert text into numerical features using TF-IDF\u001b[39;00m\n\u001b[0;32m     35\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     23\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove numbers\u001b[39;00m\n\u001b[0;32m     24\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenize\u001b[39;00m\n\u001b[0;32m     26\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m-\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# Remove stopwords except \"not\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Lemmatize words\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gadek/nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenize\n",
    "    stop_words = set(stopwords.words('english')) - {\"not\"}  # Remove stopwords except \"not\"\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatize words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "print(df[\"text\"])\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Convert text into numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"cleaned_text\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Save processed data (optional)\n",
    "df.to_csv(\"processed_data.csv\", index=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7abdfd-5150-43d2-bbac-1c50ad85bca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Sentence tokenizer\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagging (if needed)\n",
    "nltk.download('wordnet')  # WordNet Lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eef2b25c-2ced-444f-a60e-63a1ebff8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Find the nltk_data directory\n",
    "nltk_path = os.path.expanduser(\"~/nltk_data\")\n",
    "\n",
    "# Delete the folder (if it exists)\n",
    "if os.path.exists(nltk_path):\n",
    "    shutil.rmtree(nltk_path)\n",
    "\n",
    "# Reinstall all necessary datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a3f503d-8a2a-48aa-8d8f-656c3cb34b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'text.']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample text.\"\n",
    "tokens = text.split()  # Alternative to word_tokenize\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83436c45-ef9e-402e-8847-a5b9bc885460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english')[:])  # Should print a list of common stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff8ef6a6-63ec-41dc-b19e-0b14fc162d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gadek/nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt share your OTP, it is a scam.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Tokenize\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Remove stopwords but keep negations\u001b[39;00m\n\u001b[0;32m     24\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m filtered_stopwords]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gadek/nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define words to keep (negation words)\n",
    "negation_words = {\"not\", \"no\", \"never\", \"don't\", \"doesn't\", \"didn't\", \"isn't\", \"wasn't\", \"won't\", \"can't\", \"shouldn't\", \"wouldn't\"}\n",
    "\n",
    "# Remove negation words from stopwords list\n",
    "filtered_stopwords = stop_words - negation_words\n",
    "\n",
    "# Example sentence\n",
    "text = \"Don't share your OTP, it is a scam.\"\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text.lower())\n",
    "\n",
    "# Remove stopwords but keep negations\n",
    "filtered_words = [word for word in words if word not in filtered_stopwords]\n",
    "\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Filtered Words:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4a3cf74-56dc-471f-8b9d-a08fdd443b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt tokenizer is missing, please reinstall it using nltk.download('punkt').\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "try:\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    print(\"Punkt tokenizer is properly installed!\")\n",
    "except LookupError:\n",
    "    print(\"Punkt tokenizer is missing, please reinstall it using nltk.download('punkt').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14726d56-4eb9-40d0-83f7-290e481a2006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gadek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gadek/nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Verify installation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/english.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPunkt tokenizer is now properly installed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:823\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    821\u001b[0m fil \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(path_[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m])[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mswitch_punkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkers/maxent_ne_chunker\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m switch_chunker(fil\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:678\u001b[0m, in \u001b[0;36mswitch_punkt\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;124;03mReturn a pickle-free Punkt tokenizer instead of loading a pickle.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;124;03m['Hello!', 'How are you?']\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PunktTokenizer \u001b[38;5;28;01mas\u001b[39;00m tok\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtok\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gadek/nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gadek\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c599f3-db51-489b-a7b9-de740b313350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stopwords and keep \"not\"\n",
    "stop_words = set(stopwords.words('english')) - {\"not\"}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = word_tokenize(text)  # Tokenize text\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]  # Remove stopwords, keep \"not\"\n",
    "    \n",
    "    return \" \".join(words)  # Join words back into a string\n",
    "\n",
    "# Example usage\n",
    "text = \"Don't share your OTP, it is a scam.\"\n",
    "cleaned_text = preprocess_text(text)\n",
    "print(cleaned_text)  # Output: \"not share otp scam\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d6ab9-b031-485b-95df-9692aeaa85de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca133f-1857-4dde-a572-604e65ff9af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ba729-1269-4aa4-ad79-d2a39394831b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
